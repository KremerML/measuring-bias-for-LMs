{"cells":[{"cell_type":"markdown","source":["# Comparing bias measures on multiple model checkpoints\n","\n","In this notebook, we loop over the model checkpoints in a directory and assess its bias using different measures (which need to be configured first)."],"metadata":{"id":"O9htDoXQhjPs"}},{"cell_type":"markdown","source":["Adapt the file paths and execute this cell to mount Google Drive (needed if saved checkpoints are to be used)."],"metadata":{"id":"_aknmAklh3Fh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVgkXL6mE-Lm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd \"/content/drive/My Drive/NLP2_proj2/experiments\"\n","!ls"]},{"cell_type":"markdown","source":["Install dependencies. Only *transformers* and *datasets* are needed. Additionally, *torch* needs to be installed if not available on your system."],"metadata":{"id":"RCfWNslPh91R"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6wbDCIEFFA6"},"outputs":[],"source":["!pip install transformers==4.28.0\n","!pip install datasets"]},{"cell_type":"markdown","source":["Execute this cell to import dependencies."],"metadata":{"id":"jKyqXfw1iKR2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JS5nbmkXFHoC"},"outputs":[],"source":["from datasets import Dataset, load_dataset, concatenate_datasets\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n","\n","import torch\n","import os\n","import pandas as pd"]},{"cell_type":"markdown","source":["Load and preprocess CrowS-Pairs."],"metadata":{"id":"31xUxsqviOF-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQPEH8H0-kGZ"},"outputs":[],"source":["crows = load_dataset(\"BigScienceBiasEval/crows_pairs_multilingual\")\n","crows = crows[\"test\"]\n","\n","print(crows)\n","\n","crows_types = {row[\"bias_type\"] for row in crows}\n","print(crows_types)\n","\n","crows_sup = crows.filter(lambda example: example[\"bias_type\"] in [\"nationality\", \"race-color\", \"religion\"])\n","print(crows_sup)"]},{"cell_type":"markdown","source":["Load and preprocess StereoSet."],"metadata":{"id":"HxQtLZHpiRyy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIZ2f2HFLq7g"},"outputs":[],"source":["stereoset = load_dataset(\"stereoset\", \"intrasentence\")\n","stereoset = stereoset[\"validation\"]\n","print(stereoset)\n","def remap_stereoset(example):\n","    sents = example[\"sentences\"][\"sentence\"]\n","    labels = example[\"sentences\"][\"gold_label\"]\n","\n","    zipped = list(zip(sents, labels))\n","    sorted_zipped = sorted(zipped, key=lambda x: x[1])\n","    sents = [sent for sent, _ in sorted_zipped]\n","    \n","    example[\"sent_more\"] = sents[1]\n","    example[\"sent_less\"] = sents[0]\n","    return example\n","\n","stereoset = stereoset.map(remap_stereoset, remove_columns=[\"id\", \"target\", \"context\", \"sentences\"])\n","\n","stereoset_types = {row[\"bias_type\"] for row in stereoset}\n","print(stereoset_types)\n","\n","stereoset_sup = stereoset.filter(lambda example: example[\"bias_type\"] in [\"religion\", \"race\"])\n","print(stereoset_sup)"]},{"cell_type":"markdown","source":["Load and preprocess WinoBias."],"metadata":{"id":"3_gwn5igiU_-"}},{"cell_type":"code","source":["wino_pro = load_dataset(\"wino_bias\", \"type1_pro\")\n","wino_anti = load_dataset(\"wino_bias\", \"type1_anti\")\n","\n","wino_pro = concatenate_datasets([wino_pro[\"validation\"], wino_pro[\"test\"]])\n","wino_anti = concatenate_datasets([wino_anti[\"validation\"], wino_anti[\"test\"]])\n","\n","wino_pro = list(\" \".join(row[\"tokens\"]) for row in wino_pro)\n","wino_anti = list(\" \".join(row[\"tokens\"]) for row in wino_anti)\n","\n","wino = {\n","    \"sent_more\": wino_pro,\n","    \"sent_less\": wino_anti\n","}\n","\n","wino = Dataset.from_dict(wino)\n","print(wino)"],"metadata":{"id":"nKOUAa9C6557"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define two functions:\n","* *evaluation* calculates a model's perplexity on a pair of sequences and assesses which sequence is preferred by the model (the one whose perplexity is lower!).\n","* *compute_relative_preference* is a convenience function that computes the relative amount of how often the model prefers sequence 1 or 2, or none of them (if, in a highly rare case, the perplexities should be the same)."],"metadata":{"id":"7qiS8UA3pG7G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5fMkg3hCKkL"},"outputs":[],"source":["def evaluation(example):\n","    pro = torch.LongTensor(tokenizer.encode(example[\"sent_more\"])).to(\"cuda\")\n","    anti = torch.LongTensor(tokenizer.encode(example[\"sent_less\"])).to(\"cuda\")\n","    with torch.no_grad():\n","        output_pro = model(pro, labels=pro)\n","        output_anti = model(anti, labels=anti)\n","    score_pro = -torch.exp(output_pro[\"loss\"])\n","    score_anti = -torch.exp(output_anti[\"loss\"])\n","    if score_pro < score_anti:\n","        example[\"preference\"] = \"biased\"\n","    elif score_pro > score_anti:\n","        example[\"preference\"] = \"inv_biased\"\n","    else:\n","        example[\"preference\"] = \"no_preference\"\n","    return example\n","\n","def compute_relative_preference(ds):\n","    prefer_pro = sum([1 for row in ds if row[\"preference\"] == \"biased\"])\n","    prefer_anti = sum([1 for row in ds if row[\"preference\"] == \"inv_biased\"])\n","    no_preference = sum([1 for row in ds if row[\"preference\"] == \"no_preference\"])\n","    total = len(ds)\n","    print(f\"  Preferred pro: {prefer_pro / total} / Preferred anti: {prefer_anti / total} / Preferred none: {no_preference / total}\")\n","    return prefer_pro, prefer_anti, no_preference, total"]},{"cell_type":"markdown","source":["Run the evaluation by looping through the models (so that all of them only need to be loaded once) and assessing the model on each of the datasets / measures.\\\n","(Note that this implementation treats the subsets as distinct datasets and, therefore, lets the model calculate its perplexity on some sentences more than once. Computing times could, therefore, be reduced by separating the subset only after evaluation)."],"metadata":{"id":"irfhTT1fqT4k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjyqcN3ya4Vv"},"outputs":[],"source":["## run evaluation\n","\n","model_path = \"./models\"\n","model_names = [os.path.join(model_path, model_name) for model_name in os.listdir(model_path) if model_name != \"runs\"]\n","model_names.insert(0, \"gpt2\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n","\n","scores_per_model = dict()\n","\n","for model_name in model_names:\n","    print(f\"Processing {model_name}...\")\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    scores_per_model[model_name] = dict()\n","\n","    print(f\"  Evaluating on CrowS...\")\n","    crows = crows.map(evaluation)\n","    scores_per_model[model_name][\"crows\"] = compute_relative_preference(crows)\n","    print(f\"  Evaluating on CrowS_sup...\")\n","    crows_sup = crows_sup.map(evaluation)\n","    scores_per_model[model_name][\"crows_sup\"] = compute_relative_preference(crows_sup)\n","    print(f\"  Evaluating on StereoSet...\")\n","    stereoset = stereoset.map(evaluation)\n","    scores_per_model[model_name][\"stereoset\"] = compute_relative_preference(stereoset)\n","    print(f\"  Evaluating on StereoSet_sup...\")\n","    stereoset_sup = stereoset_sup.map(evaluation)\n","    scores_per_model[model_name][\"stereoset_sup\"] = compute_relative_preference(stereoset_sup)\n","    print(f\"  Evaluating on WinoBias...\")\n","    wino = wino.map(evaluation)\n","    scores_per_model[model_name][\"wino\"] = compute_relative_preference(wino)"]},{"cell_type":"markdown","source":["The above cell saved all scores to a nested dictionary. Use one of the below code blocks to display the scores either per model or per measure.\n","\n"],"metadata":{"id":"D4ejPCCxrEoC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bmSb3flhbLl"},"outputs":[],"source":["measures = [\"crows\", \"crows_sup\", \"stereoset\", \"stereoset_sup\", \"wino\"]\n","\n","for i, measure in enumerate(measures):\n","    print(f\"Measure: {measure}\")\n","    for model in scores_per_model.keys():\n","        values = scores_per_model[model][measure]\n","        total = values[3]\n","        print(f\"  {(round(values[0]/total, 3), round(values[1]/total, 3), values[2]/total, total)}\")\n","        ##print(f\"  {scores_per_model[model][measure]}\")\n","\n","#for model_name in model_names:\n","#    print(\"-\" * 10)\n","#    print(model_name)\n","#    for measure in scores_per_model[model_name]:\n","#        print(measure)\n","#        values = scores_per_model[model_name][measure]\n","#        total = values[3]\n","#        print(f\"Scores for {measure}:\\n  Preferred pro: {values[0] / total} / Preferred anti: {values[1] / total} / Preferred none: {values[2] / total}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}