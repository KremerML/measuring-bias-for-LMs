{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Fine-tuning GPT2 to control inherent bias\n","\n","In this notebook, we fine-tune the pretrained GPT2 model provided in the *transformers* library on an aggregated hate speech dataset, assuming that longer training on hate speech data introduces a higher bias."],"metadata":{"id":"7mieeFfKWC56"}},{"cell_type":"markdown","source":["Adapt the file paths and execute this cell to mount Google Drive (needed if checkpoints are to be saved)."],"metadata":{"id":"D8WBLsKgWX60"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd \"/content/drive/My Drive/NLP2_proj2/experiments\"\n","!ls"],"metadata":{"id":"q6FiAnvtzk5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install dependencies. Only *transformers* and *datasets* are necessarily needed. *accelerate* is necessary if the models are to be fine-tuned on GPU (recommended!). Additionally, *torch* needs to be installed if not available on your system."],"metadata":{"id":"QCouwXtFWiO3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmjQZvhgER_K"},"outputs":[],"source":["!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install accelerate"]},{"cell_type":"markdown","source":["Execute this cell to import dependencies."],"metadata":{"id":"hEH6JQWQWvp2"}},{"cell_type":"code","source":["from datasets import load_dataset, concatenate_datasets\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, pipeline\n","\n","import math\n","import torch\n","from tqdm import tqdm"],"metadata":{"id":"CbHNtbB5FKFA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the pretrained GPT3 model and the corresponding tokenizer."],"metadata":{"id":"mclUFXNoXIsi"}},{"cell_type":"code","source":["pretrained_model = \"gpt2\"\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast=True)\n","model = AutoModelForCausalLM.from_pretrained(pretrained_model)"],"metadata":{"id":"YXBro03azxKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the datasets, clean them, and concatenate them."],"metadata":{"id":"w7uNQfUXgKGL"}},{"cell_type":"code","source":["# load and clean\n","\n","hs18 = load_dataset(\"hate_speech18\")\n","hs18 = hs18[\"train\"]\n","hs18 = hs18.filter(lambda example: example[\"label\"]==1)\n","hs18 = hs18.remove_columns([\"user_id\", \"subforum_id\", \"num_contexts\", \"label\"])\n","print(hs18)\n","\n","frenk = load_dataset(\"classla/FRENK-hate-en\", \"binary\")\n","frenk = concatenate_datasets([frenk[\"train\"], frenk[\"validation\"], frenk[\"test\"]])\n","frenk = frenk.filter(lambda example: example[\"label\"]==1)\n","frenk = frenk.remove_columns([\"target\", \"topic\", \"label\"])\n","frenk = frenk.shuffle()\n","print(frenk)\n","\n","def detokenize_he(example):\n","    example[\"text\"] = \" \".join(example[\"post_tokens\"])\n","    return example\n","\n","he = load_dataset(\"hatexplain\")\n","he = concatenate_datasets([he[\"train\"], he[\"validation\"], he[\"test\"]])\n","he = he.filter(lambda example: any([True for annotation in example[\"annotators\"][\"label\"] if annotation>0]))\n","he = he.map(detokenize_he)\n","he = he.remove_columns([\"id\", \"annotators\", \"rationales\", \"post_tokens\"])\n","he = he.shuffle()\n","print(he)\n","\n","## concatenate\n","\n","#hate_set = concatenate_datasets([hs18, frenk, he.select(range(4554))])  ## use this line instead of the following one for a smaller, more balanced-out dataset (size=10k)\n","hate_set = concatenate_datasets([hs18, frenk, he])\n","hate_set.shuffle"],"metadata":{"id":"go1a_Y1kzI8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess the combined training set (by creating larger contexts and reducing the number of sequences). Create train/test split of 95:5."],"metadata":{"id":"df6VxmqCgctd"}},{"cell_type":"code","source":["def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"])\n","\n","hate_set_tok = hate_set.map(tokenize_function, batched=True, num_proc=4, remove_columns = [\"text\"])\n","\n","hate_set_tok = hate_set_tok.train_test_split(test_size=0.05)\n","\n","block_size = 128\n","\n","def group_texts(examples):\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    total_length = (total_length // block_size) * block_size\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","hate_set_lm = hate_set_tok.map(\n","    group_texts,\n","    batched=True,\n","    batch_size=1000,\n","    num_proc=4,\n",")\n","\n","print(hate_set_lm)"],"metadata":{"id":"mXRuNUc3FM5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Configure hyperparameters for fine-tuning."],"metadata":{"id":"Adwm4XU2grmA"}},{"cell_type":"code","source":["model_dir = \"/content/drive/My Drive/NLP2_proj2/experiments/models\"\n","\n","model_name = pretrained_model\n","\n","training_args = TrainingArguments(\n","    output_dir = model_dir,\n","    evaluation_strategy = \"steps\",\n","    learning_rate=1e-5,\n","    warmup_steps=50,\n","    weight_decay=0.01,\n","    max_steps = 450,\n","    per_device_train_batch_size=32,\n","    save_strategy = \"steps\", \n","    logging_steps = 50,\n","    save_steps = 50,\n","    save_total_limit = 10\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=hate_set_lm[\"train\"],\n","    eval_dataset=hate_set_lm[\"test\"]\n",")"],"metadata":{"id":"GC4xf-QyIaki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine-tune the model."],"metadata":{"id":"ToW9ndPPgv-C"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"4SolXrk_JDy-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the model's perplexity on the validation set."],"metadata":{"id":"QxMFDFPTgyqg"}},{"cell_type":"code","source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"metadata":{"id":"73a6cFwPO4Nl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialise generator object to generate sequences using the fine-tuned model."],"metadata":{"id":"PST_Qbdkg8ll"}},{"cell_type":"code","source":["generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"],"metadata":{"id":"T1BNr82fLaeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define beginnings of sequences and let the model continue them."],"metadata":{"id":"jZowbgoQhBog"}},{"cell_type":"code","source":["prompts = [\"She\", \"He\", \"The woman\", \"The men\", \"The doctor\", \"The nurse\", \"The black\", \"The white\", \"The muslim\", \"The african\", \"The latino\"]\n","\n","for prompt in prompts:\n","    print(\"-\" * 10)\n","    print(prompt)\n","    for output in generator(prompt, max_length=40, num_return_sequences=3):\n","        sequence = output[\"generated_text\"]\n","        print(f\"    {sequence}\")\n","        print(\"    \" + \"-\" * 5)"],"metadata":{"id":"FYtx8XoSLiks"},"execution_count":null,"outputs":[]}]}