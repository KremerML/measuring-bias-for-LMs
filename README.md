
# Assessing the suitability of CrowS-Pairs as a bias measure in LLMs

This is the official repository of a study performed at the University of Amsterdam during the course "Natural Language Processing II" in 2023.

[[Project report]](insert_link_when_available)

## Abstract

The use of large language models is becoming increasingly widespread across various domains. Yet, these models frequently exhibit biases towards marginalized or less-represented communities, posing a significant concern on their direct deployment. As of now, there is an absence of a universally accepted standard for quantifying the bias of these language models, although there exist several metrics, each having unique characteristics. This paper delves into the examination of one such metric, CrowS-Pairs (Nangia et al., 2020), testing its reliability and validity on BERT and GPT-based models. Our findings suggest the metric is valid and reliable for measuring bias.

*(Find the entire paper using the link provided above)*

## Reproducibility

All our experiments (including implementations, dependencies, and loaded datasets and models) are made available in Jupyter notebooks in [notebooks](/notebooks).

## Questions

Feel free to message us using the addresses provided in the report.
